# object_reaching

## Description
This package provides the functionality for an object reaching game. Two objects are placed in front of a human participant and two other objects in front of a robotic manipulator. The human tries to approach one of the objects and through a prediction module we predict the object the human tries to reach. Based on the prediction, the robotic manipulator hits one of its objects.

## Pipeline
* **Human monitoring**: An RGB-D camera is used for the human monitoring and [Openpose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) is utilized for the 2D human localization. The wrist pixels of the right hand are used for predicting the object the human tries to approach.

* **Human motion preprocessing**: The human movements (2D Openpose pixels) are first filtered:
	* Removal of pixels corresponding to the rest at the beginning of the motion.
	* Removal of outliers. 

* **Prediction**: Pretrained models are used. The predition of the direction of the human motion is based on his distance from the target objects.

* **Robot motion**: The robot moves to a direction according to the output of the human motion prediction module.

## Files
* `scripts/input_process.py`: Movement detection onset and end and movement filtering.
* `scripts/prediction.py`: Prediction of human movement direction.
* `src/robot_motion.cpp`: Robot motion generation.
* `scripts/result.py`: Checks who reached the object first (human or robot).
* `config/prediction.yaml`: Set the positions (pixels) of the human objects.
* `config/object_reaching.yaml`: Set the positions (3D coordinates expressed in the base_link frame) of the robot objects, the initial position of the robot, the velocity with which the robot will hit the objects and the gain of the controller used for regulating the robot commanded velocities.
 
## Run
Run `roslaunch object_reaching object_reaching.launch` to launch the [OpenPose ROS node](https://github.com/firephinx/openpose_ros) used for the human monitoring, the motion detection node, the prediction node and the robot motion node.

Once the nodes have been launched, in another terminal run `rosservice call /next_motion`. This service call will initialize the loop of the game, namely the motion detection node will listen to the 2D output of the OpenPose node. At the end of the game, namely when the robot hits an object, the robot will stay still for 5 seconds and then automatically return to its initial position. Then, run again `rosservice call /next_motion` to start the second experiment. 

To launch the real robot, check the Roboskel's [UR3 repo](https://github.com/Roboskel-Manipulation/manos).

## Arguments
* visual_input: True if using visual input to produce the 2D pixels either using the real camera or a rosbag. False if using already obtained 2D pixels.
* live_camera: True if frames are generated by an RGB-D camera (False if they are generated by rosbags)
* models_path: Set the absolute path of the models used for the prediction

NOTE: `live_camera` need to be set only if `visual_input` has been set to true.
